{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/ptcheat.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                        # root package\n",
    "from torch.utils.data import Dataset, DataLoader    # dataset representation and loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "from torch.jit import script, trace       # hybrid frontend decorator and tracing jit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchscript and JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.trace()         # takes your module or function and an example\n",
    "                          # data input, and traces the computational steps\n",
    "                          # that the data encounters as it progresses through the model\n",
    "\n",
    "@script                   # decorator used to indicate data-dependent\n",
    "                          # control flow within the code being traced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, dummy data, xxxx.proto)       # exports an ONNX formatted\n",
    "                                                       # model using a trained model, dummy\n",
    "                                                       # data and the desired file name\n",
    "\n",
    "model = onnx.load(\"alexnet.proto\")                     # load an ONNX model\n",
    "onnx.checker.check_model(model)                        # check that the model\n",
    "                                                       # IR is well formed\n",
    "\n",
    "onnx.helper.printable_graph(model.graph)               # print a human readable\n",
    "                                                       # representation of the graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms     # vision datasets,\n",
    "                                                         # architectures &\n",
    "                                                         # transforms\n",
    "\n",
    "import torchvision.transforms as transforms              # composable transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist             # distributed communication\n",
    "from torch.multiprocessing import Process    # memory sharing processes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(*size)              # tensor with independent N(0,1) entries\n",
    "x = torch.[ones|zeros](*size)       # tensor with all 1's [or 0's]\n",
    "x = torch.tensor(L)                 # create tensor from [nested] list or ndarray L\n",
    "y = x.clone()                       # clone of x\n",
    "with torch.no_grad():               # code wrap that stops autograd from tracking tensor history\n",
    "requires_grad=True                  # arg, when set to True, tracks computation\n",
    "                                    # history for future derivative calculations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()                                  # return tuple-like object of dimensions\n",
    "x = torch.cat(tensor_seq, dim=0)          # concatenates tensors along dim\n",
    "y = x.view(a,b,...)                       # reshapes x into size (a,b,...)\n",
    "y = x.view(-1,a)                          # reshapes x into size (b,a) for some b\n",
    "y = x.transpose(a,b)                      # swaps dimensions a and b\n",
    "y = x.permute(*dims)                      # permutes dimensions\n",
    "y = x.unsqueeze(dim)                      # tensor with added axis\n",
    "y = x.unsqueeze(dim=2)                    # (a,b,c) tensor -> (a,b,1,c) tensor\n",
    "y = x.squeeze()                           # removes all dimensions of size 1 (a,1,b,1) -> (a,b)\n",
    "y = x.squeeze(dim=1)                      # removes specified dimension of size 1 (a,1,b,1) -> (a,b,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = A.mm(B)       # matrix multiplication\n",
    "ret = A.mv(x)       # matrix-vector multiplication\n",
    "x = x.t()           # matrix transpose"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available                                     # check for cuda\n",
    "x = x.cuda()                                                # move x's data from\n",
    "                                                            # CPU to GPU and return new object\n",
    "\n",
    "x = x.cpu()                                                 # move x's data from GPU to CPU\n",
    "                                                            # and return new object\n",
    "\n",
    "if not args.disable_cuda and torch.cuda.is_available():     # device agnostic code\n",
    "    args.device = torch.device('cuda')                      # and modularity\n",
    "else:                                                       #\n",
    "    args.device = torch.device('cpu')                       #\n",
    "\n",
    "net.to(device)                                              # recursively convert their\n",
    "                                                            # parameters and buffers to\n",
    "                                                            # device specific tensors\n",
    "\n",
    "x = x.to(device)                                            # copy your tensors to a device\n",
    "                                                            # (gpu, cpu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear(m,n)                                # fully connected layer from\n",
    "                                              # m to n units\n",
    "\n",
    "nn.ConvXd(m,n,s)                              # X dimensional conv layer from\n",
    "                                              # m to n channels where X‚ç∑{1,2,3}\n",
    "                                              # and the kernel size is s\n",
    "\n",
    "nn.MaxPoolXd(s)                               # X dimension pooling layer\n",
    "                                              # (notation as above)\n",
    "\n",
    "nn.BatchNormXd                                # batch norm layer\n",
    "nn.RNN/LSTM/GRU                               # recurrent layers\n",
    "nn.Dropout(p=0.5, inplace=False)              # dropout layer for any dimensional input\n",
    "nn.Dropout2d(p=0.5, inplace=False)            # 2-dimensional channel-wise dropout\n",
    "nn.Embedding(num_embeddings, embedding_dim)   # (tensor-wise) mapping from\n",
    "                                              # indices to embedding vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.X                                  # where X is L1Loss, MSELoss, CrossEntropyLoss\n",
    "                                      # CTCLoss, NLLLoss, PoissonNLLLoss,\n",
    "                                      # KLDivLoss, BCELoss, BCEWithLogitsLoss,\n",
    "                                      # MarginRankingLoss, HingeEmbeddingLoss,\n",
    "                                      # MultiLabelMarginLoss, SmoothL1Loss,\n",
    "                                      # SoftMarginLoss, MultiLabelSoftMarginLoss,\n",
    "                                      # CosineEmbeddingLoss, MultiMarginLoss,\n",
    "                                      # or TripletMarginLoss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.X                                  # where X is ReLU, ReLU6, ELU, SELU, PReLU, LeakyReLU,\n",
    "                                      # RReLu, CELU, GELU, Threshold, Hardshrink, HardTanh,\n",
    "                                      # Sigmoid, LogSigmoid, Softplus, SoftShrink,\n",
    "                                      # Softsign, Tanh, TanhShrink, Softmin, Softmax,\n",
    "                                      # Softmax2d, LogSoftmax or AdaptiveSoftmaxWithLoss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.x(model.parameters(), ...)      # create optimizer\n",
    "opt.step()                                  # update weights\n",
    "optim.X                                     # where X is SGD, Adadelta, Adagrad, Adam,\n",
    "                                            # AdamW, SparseAdam, Adamax, ASGD,\n",
    "                                            # LBFGS, RMSprop or Rprop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.X(optimizer,...)      # create lr scheduler\n",
    "scheduler.step()                        # update lr after optimizer updates weights\n",
    "optim.lr_scheduler.X                    # where X is LambdaLR, MultiplicativeLR,\n",
    "                                        # StepLR, MultiStepLR, ExponentialLR,\n",
    "                                        # CosineAnnealingLR, ReduceLROnPlateau, CyclicLR,\n",
    "                                        # OneCycleLR, CosineAnnealingWarmRestarts,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset                    # abstract class representing dataset\n",
    "TensorDataset              # labelled dataset in the form of tensors\n",
    "Concat Dataset             # concatenation of Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders and DataSamplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader(dataset, batch_size=1, ...)      # loads data batches agnostic\n",
    "                                            # of structure of individual data points\n",
    "\n",
    "sampler.Sampler(dataset,...)                # abstract class dealing with\n",
    "                                            # ways to sample from dataset\n",
    "\n",
    "sampler.XSampler where ...                  # Sequential, Random, SubsetRandom,\n",
    "                                            # WeightedRandom, Batch, Distributed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f49206fcf84a9145e7e21228cbafa911d1ac18292303b01e865d8267a9c448f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
